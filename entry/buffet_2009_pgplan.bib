@article{buffet_2009_pgplan,
title = "The factored policy-gradient planner",
journal = "Artificial Intelligence",
volume = "173",
number = "5",
pages = "722 - 747",
year = "2009",
note = "Advances in Automated Plan Generation",
issn = "0004-3702",
doi = "https://doi.org/10.1016/j.artint.2008.11.008",
url = "http://www.sciencedirect.com/science/article/pii/S0004370208001859",
author = "Olivier Buffet and Douglas Aberdeen",
keywords = "Concurrent probabilistic temporal planning, Reinforcement learning, Policy-gradient, AI planning",
abstract = "We present an any-time concurrent probabilistic temporal planner (CPTP) that includes continuous and discrete uncertainties and metric functions. Rather than relying on dynamic programming our approach builds on methods from stochastic local policy search. That is, we optimise a parameterised policy using gradient ascent. The flexibility of this policy-gradient approach, combined with its low memory use, the use of function approximation methods and factorisation of the policy, allow us to tackle complex domains. This factored policy gradient (FPG) planner can optimise steps to goal, the probability of success, or attempt a combination of both. We compare the FPG planner to other planners on CPTP domains, and on simpler but better studied non-concurrent non-temporal probabilistic planning (PP) domains. We present FPG-ipc, the PP version of the planner which has been successful in the probabilistic track of the fifth international planning competition."
}
