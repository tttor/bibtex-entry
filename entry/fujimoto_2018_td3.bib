@InProceedings{fujimoto_2018_td3,
  title =    {Addressing Function Approximation Error in Actor-Critic Methods},
  author =   {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle =    {Proceedings of the 35th International Conference on Machine Learning},
  pages =    {1587--1596},
  year =   {2018},
  editor =   {Dy, Jennifer and Krause, Andreas},
  volume =   {80},
  series =   {Proceedings of Machine Learning Research},
  address =    {Stockholmsm√§ssan, Stockholm Sweden},
  month =    {10--15 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf},
  url =    {http://proceedings.mlr.press/v80/fujimoto18a.html},
  abstract =   {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.}
}
