@Article{kearns_2002_eee,
author="Kearns, Michael
and Singh, Satinder",
title="Near-Optimal Reinforcement Learning in Polynomial Time",
journal="Machine Learning",
year="2002",
month="Nov",
day="01",
volume="49",
number="2",
pages="209--232",
abstract="We present new algorithms for reinforcement learning and prove that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes. After observing that the number of actions required to approach the optimal return is lower bounded by the mixing time T of the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give algorithms requiring a number of actions and total computation time that are only polynomial in T and the number of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is their explicit handling of the Exploration-Exploitation trade-off.",
issn="1573-0565",
doi="10.1023/A:1017984413808",
url="https://doi.org/10.1023/A:1017984413808"
}
