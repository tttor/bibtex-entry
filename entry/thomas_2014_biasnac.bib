@InProceedings{thomas_2014_biasnac,
  title =    {Bias in Natural Actor-Critic Algorithms},
  author =   {Philip Thomas},
  booktitle =    {Proceedings of the 31st International Conference on Machine Learning},
  pages =    {441--448},
  year =     {2014},
  editor =   {Eric P. Xing and Tony Jebara},
  volume =   {32},
  number =       {1},
  series =   {Proceedings of Machine Learning Research},
  address =      {Bejing, China},
  month =    {22--24 Jun},
  publisher =    {PMLR},
  pdf =      {http://proceedings.mlr.press/v32/thomas14.pdf},
  url =      {http://proceedings.mlr.press/v32/thomas14.html},
  abstract =     {We show that several popular discounted reward natural actor-critics, including the popular NAC-LSTD and eNAC algorithms, do not generate unbiased estimates of the natural policy gradient as claimed. We derive the first unbiased discounted reward natural actor-critics using batch and iterative approaches to gradient estimation. We argue that the bias makes the existing algorithms more appropriate for the average reward setting. We also show that, when Sarsa(lambda) is guaranteed to converge to an optimal policy, the objective function used by natural actor-critics is concave, so policy gradient methods are guaranteed to converge to globally optimal policies as well.}
}
